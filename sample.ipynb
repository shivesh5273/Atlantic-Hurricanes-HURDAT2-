{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Rapid Intensification (RI) Prediction — Atlantic Hurricanes (HURDAT2)\n",
    "\n",
    "**Goal:** Build a machine learning classifier that predicts whether a tropical cyclone will **rapidly intensify** within the next 24 hours, using only information available *up to the current observation time*.\n",
    "\n",
    "**Definition (label):**\n",
    "Rapid Intensification (RI) = **maximum sustained wind increases by ≥ 30 knots within 24 hours**.\n",
    "\n",
    "**Dataset:** Kaggle — Atlantic Hurricane Dataset (HURDAT2), cleaned CSV format (`hurdat2.csv`)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = Path(\"hurdat2.csv\")  # put hurdat2.csv in same folder as notebook\n"
   ],
   "id": "fbc121e30a2defb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ],
   "id": "48300bc1b10ca08e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset Overview\n",
    "\n",
    "This dataset contains storm track observations (typically 6-hour intervals).\n",
    "Each row is an observation for a storm at a timestamp, including:\n",
    "\n",
    "- **Storm metadata:** `storm_id`, `storm_name`\n",
    "- **Time:** `date`, `time`\n",
    "- **Location:** `latitude`, `longitude`\n",
    "- **Intensity:** `maximum_sustained_wind_knots` (target driver), and often pressure-related fields if present\n",
    "- **Storm status:** `status_of_system` (e.g., TS, HU)\n",
    "\n",
    "We will:\n",
    "1. Clean and parse timestamps.\n",
    "2. Convert latitude/longitude strings to numeric.\n",
    "3. Create an RI label from wind change over the next 24 hours.\n",
    "4. Train multiple ML models using proper storm-wise splits."
   ],
   "id": "e7f8f8dfd96b0ebc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.info()\n",
    "\n",
    "missing_pct = (df.isna().mean() * 100).sort_values(ascending=False)\n",
    "missing_pct.head(20)"
   ],
   "id": "dbbd093f503a6038"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_lat(lat):\n",
    "    # often like \"18.0N\" or \"18.0\"\n",
    "    if pd.isna(lat):\n",
    "        return np.nan\n",
    "    s = str(lat).strip()\n",
    "    if s[-1] in [\"N\", \"S\"]:\n",
    "        val = float(s[:-1])\n",
    "        return val if s[-1] == \"N\" else -val\n",
    "    return float(s)\n",
    "\n",
    "def parse_lon(lon):\n",
    "    # often like \"65.0W\" or \"65.0\"\n",
    "    if pd.isna(lon):\n",
    "        return np.nan\n",
    "    s = str(lon).strip()\n",
    "    if s[-1] in [\"E\", \"W\"]:\n",
    "        val = float(s[:-1])\n",
    "        return val if s[-1] == \"E\" else -val\n",
    "    return float(s)\n",
    "\n",
    "# timestamp: date as YYYYMMDD and time as HHMM (or sometimes integer like 0, 600, 1200, 1800)\n",
    "df[\"date\"] = df[\"date\"].astype(str)\n",
    "df[\"time\"] = df[\"time\"].astype(str).str.zfill(4)\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"date\"] + df[\"time\"], format=\"%Y%m%d%H%M\", errors=\"coerce\", utc=True)\n",
    "\n",
    "df[\"lat\"] = df[\"latitude\"].apply(parse_lat)\n",
    "df[\"lon\"] = df[\"longitude\"].apply(parse_lon)\n",
    "\n",
    "WIND_COL = \"maximum_sustained_wind_knots\"\n",
    "df[WIND_COL] = pd.to_numeric(df[WIND_COL], errors=\"coerce\").replace(-99, np.nan)\n",
    "\n",
    "print(df[[\"storm_id\",\"storm_name\",\"timestamp\",\"lat\",\"lon\",WIND_COL]].head())\n",
    "print(\"Timestamp null %:\", df[\"timestamp\"].isna().mean()*100)"
   ],
   "id": "14cdbcaad891d6a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "We will look at:\n",
    "- distribution of wind speed\n",
    "- storm status breakdown\n",
    "- geographic scatter plot\n",
    "- time coverage"
   ],
   "id": "3140d7ea5e4bb2e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "df[WIND_COL].dropna().hist(bins=50, ax=ax)\n",
    "ax.set_title(\"Maximum Sustained Wind (knots) distribution\")\n",
    "ax.set_xlabel(\"knots\")\n",
    "ax.set_ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "df[\"status_of_system\"].value_counts().head(15)"
   ],
   "id": "eae48e2ec52ff1a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample = df.dropna(subset=[\"lat\",\"lon\"]).sample(n=min(5000, len(df)), random_state=RANDOM_SEED)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(sample[\"lon\"], sample[\"lat\"], s=3)\n",
    "ax.set_title(\"Track point locations (sample)\")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "plt.show()"
   ],
   "id": "dabbb3e525f83882"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Label Construction: Rapid Intensification (RI)\n",
    "\n",
    "We create a binary label **RI_24h**:\n",
    "\n",
    "- For each storm observation at time *t*, we find the wind at time *t + 24 hours* (or closest observation at that horizon).\n",
    "- If `wind(t+24h) - wind(t) >= 30 knots`, then **RI_24h = 1**, else 0.\n",
    "\n",
    "**Important:** Our features must use only data available at or before time *t* to avoid leakage."
   ],
   "id": "a4290d6d57e09190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df.sort_values([\"storm_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# We'll approximate \"24h ahead\" using 4 steps of 6-hourly data.\n",
    "# But because some storms have irregularities, we do a safer merge using shift by 4 within each storm.\n",
    "# This is a common and reasonable approach for HURDAT2 6-hourly tracks.\n",
    "\n",
    "df[\"wind_t\"] = df[WIND_COL]\n",
    "df[\"wind_t_plus_24h\"] = df.groupby(\"storm_id\")[\"wind_t\"].shift(-4)  # 4*6h = 24h\n",
    "df[\"delta_wind_24h\"] = df[\"wind_t_plus_24h\"] - df[\"wind_t\"]\n",
    "\n",
    "df[\"RI_24h\"] = (df[\"delta_wind_24h\"] >= 30).astype(int)\n",
    "\n",
    "# Drop rows where label can't be computed (end of storm) or wind missing\n",
    "model_df = df.dropna(subset=[\"wind_t\",\"wind_t_plus_24h\",\"timestamp\",\"lat\",\"lon\"]).copy()\n",
    "\n",
    "print(\"Model rows:\", model_df.shape)\n",
    "print(\"RI positive rate:\", model_df[\"RI_24h\"].mean())\n",
    "model_df[[\"storm_id\",\"timestamp\",\"wind_t\",\"wind_t_plus_24h\",\"delta_wind_24h\",\"RI_24h\"]].head(10)"
   ],
   "id": "e96831ffe7e8e9b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We will build features that mimic a real monitoring/forecasting setup:\n",
    "\n",
    "**Current-state features**\n",
    "- wind now, lat, lon, storm status\n",
    "\n",
    "**Trend features (past-only)**\n",
    "- wind change over last 6h, 12h, 24h (computed via lagged values within each storm)\n",
    "- recent rolling mean\n",
    "\n",
    "These are simple but strong baseline signals and align with the “textbook ML workflow” before adding deep learning."
   ],
   "id": "f3a1751e6be2d46b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_lags(group):\n",
    "    g = group.sort_values(\"timestamp\").copy()\n",
    "    g[\"wind_lag_1\"] = g[\"wind_t\"].shift(1)   # 6h ago\n",
    "    g[\"wind_lag_2\"] = g[\"wind_t\"].shift(2)   # 12h ago\n",
    "    g[\"wind_lag_4\"] = g[\"wind_t\"].shift(4)   # 24h ago\n",
    "\n",
    "    g[\"dwind_6h\"]  = g[\"wind_t\"] - g[\"wind_lag_1\"]\n",
    "    g[\"dwind_12h\"] = g[\"wind_t\"] - g[\"wind_lag_2\"]\n",
    "    g[\"dwind_24h_past\"] = g[\"wind_t\"] - g[\"wind_lag_4\"]\n",
    "\n",
    "    g[\"wind_rollmean_4\"] = g[\"wind_t\"].rolling(4).mean()  # last 24h avg\n",
    "    return g\n",
    "\n",
    "feat_df = model_df.groupby(\"storm_id\", group_keys=False).apply(add_lags)\n",
    "\n",
    "# Drop rows where lag features unavailable\n",
    "feat_df = feat_df.dropna(subset=[\"wind_lag_1\",\"wind_lag_2\",\"wind_lag_4\",\"wind_rollmean_4\"]).copy()\n",
    "\n",
    "print(\"After lag features:\", feat_df.shape)\n",
    "feat_df[[\"storm_id\",\"timestamp\",\"wind_t\",\"dwind_6h\",\"dwind_12h\",\"dwind_24h_past\",\"wind_rollmean_4\",\"RI_24h\"]].head()"
   ],
   "id": "911d3f1e10bdc411"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train / Validation / Test Split (Storm-wise)\n",
    "\n",
    "We must split by **storm_id** so that observations from the same storm do not leak across train/val/test.\n",
    "\n",
    "We will do:\n",
    "- Train: 70%\n",
    "- Validation: 15%\n",
    "- Test: 15%\n",
    "\n",
    "This matches realistic generalization: predicting on **new storms**."
   ],
   "id": "a57e48421ea51ad1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = feat_df.copy()\n",
    "y = feat_df[\"RI_24h\"].astype(int).copy()\n",
    "groups = feat_df[\"storm_id\"].copy()\n",
    "\n",
    "# First split: train vs temp\n",
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.30, random_state=RANDOM_SEED)\n",
    "train_idx, temp_idx = next(gss1.split(X, y, groups=groups))\n",
    "\n",
    "train = feat_df.iloc[train_idx].copy()\n",
    "temp  = feat_df.iloc[temp_idx].copy()\n",
    "\n",
    "# Second split: val vs test from temp\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.50, random_state=RANDOM_SEED)\n",
    "val_idx, test_idx = next(gss2.split(temp, temp[\"RI_24h\"], groups=temp[\"storm_id\"]))\n",
    "\n",
    "val  = temp.iloc[val_idx].copy()\n",
    "test = temp.iloc[test_idx].copy()\n",
    "\n",
    "print(\"Train:\", train.shape, \"Val:\", val.shape, \"Test:\", test.shape)\n",
    "print(\"Train RI rate:\", train[\"RI_24h\"].mean())\n",
    "print(\"Val   RI rate:\", val[\"RI_24h\"].mean())\n",
    "print(\"Test  RI rate:\", test[\"RI_24h\"].mean())"
   ],
   "id": "3ad5b56583763f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modeling with Pipelines\n",
    "\n",
    "We will use scikit-learn Pipelines to ensure clean, reproducible preprocessing:\n",
    "- numeric: impute + scale\n",
    "- categorical: impute + one-hot encode\n",
    "\n",
    "Then we train multiple models:\n",
    "- Perceptron\n",
    "- Logistic Regression\n",
    "- SVM (linear and RBF)\n",
    "- KNN\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "\n",
    "We select the best model based on validation metrics.\n",
    "Because RI is imbalanced, we will track:\n",
    "- Accuracy (easy to inflate)\n",
    "- F1 (balance precision/recall)\n",
    "- ROC-AUC\n",
    "- PR-AUC (Average Precision) — best for imbalanced positives"
   ],
   "id": "6e538857cc636eb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "target = \"RI_24h\"\n",
    "\n",
    "feature_cols_num = [\n",
    "    \"wind_t\", \"lat\", \"lon\",\n",
    "    \"wind_lag_1\", \"wind_lag_2\", \"wind_lag_4\",\n",
    "    \"dwind_6h\", \"dwind_12h\", \"dwind_24h_past\",\n",
    "    \"wind_rollmean_4\"\n",
    "]\n",
    "\n",
    "feature_cols_cat = [\"status_of_system\"]\n",
    "\n",
    "# Some datasets might have status missing; keep safe\n",
    "for c in feature_cols_cat:\n",
    "    if c not in feat_df.columns:\n",
    "        feat_df[c] = np.nan\n",
    "        train[c] = np.nan\n",
    "        val[c] = np.nan\n",
    "        test[c] = np.nan\n",
    "\n",
    "X_train, y_train = train[feature_cols_num + feature_cols_cat], train[target]\n",
    "X_val, y_val     = val[feature_cols_num + feature_cols_cat], val[target]\n",
    "X_test, y_test   = test[feature_cols_num + feature_cols_cat], test[target]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_cols_num),\n",
    "        (\"cat\", categorical_transformer, feature_cols_cat),\n",
    "    ]\n",
    ")"
   ],
   "id": "df1526a11f40fa97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def eval_model(name, model, Xtr, ytr, Xva, yva):\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "\n",
    "    pred = pipe.predict(Xva)\n",
    "\n",
    "    # probabilities/scores for AUC\n",
    "    if hasattr(pipe.named_steps[\"model\"], \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(Xva)[:, 1]\n",
    "    elif hasattr(pipe.named_steps[\"model\"], \"decision_function\"):\n",
    "        scores = pipe.decision_function(Xva)\n",
    "        # convert scores to 0..1-ish via minmax for PR-AUC stability\n",
    "        proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
    "    else:\n",
    "        proba = None\n",
    "\n",
    "    acc = accuracy_score(yva, pred)\n",
    "    prec = precision_score(yva, pred, zero_division=0)\n",
    "    rec = recall_score(yva, pred, zero_division=0)\n",
    "    f1 = f1_score(yva, pred, zero_division=0)\n",
    "\n",
    "    roc = roc_auc_score(yva, proba) if proba is not None else np.nan\n",
    "    pr  = average_precision_score(yva, proba) if proba is not None else np.nan\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": pr,\n",
    "        \"pipeline\": pipe\n",
    "    }"
   ],
   "id": "75414799b3196e7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "models = [\n",
    "    (\"Perceptron\", Perceptron(random_state=RANDOM_SEED)),\n",
    "    (\"LogReg\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_SEED)),\n",
    "    (\"LinearSVM\", SVC(kernel=\"linear\", class_weight=\"balanced\", probability=True, random_state=RANDOM_SEED)),\n",
    "    (\"RBFSVM\", SVC(kernel=\"rbf\", class_weight=\"balanced\", probability=True, random_state=RANDOM_SEED)),\n",
    "    (\"KNN\", KNeighborsClassifier(n_neighbors=15)),\n",
    "    (\"DecisionTree\", DecisionTreeClassifier(max_depth=6, random_state=RANDOM_SEED, class_weight=\"balanced\")),\n",
    "    (\"RandomForest\", RandomForestClassifier(\n",
    "        n_estimators=300, random_state=RANDOM_SEED, class_weight=\"balanced\", n_jobs=-1\n",
    "    )),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, m in models:\n",
    "    out = eval_model(name, m, X_train, y_train, X_val, y_val)\n",
    "    results.append(out)\n",
    "\n",
    "score_table = pd.DataFrame([{k:v for k,v in r.items() if k!=\"pipeline\"} for r in results])\n",
    "score_table.sort_values(\"pr_auc\", ascending=False)"
   ],
   "id": "c26d5cd8024b95ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Selection\n",
    "\n",
    "We choose the best model based primarily on **Validation PR-AUC**, with F1 as a secondary metric.\n",
    "PR-AUC is ideal because RI is relatively rare (imbalanced classification)."
   ],
   "id": "917cb9224dc92a0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best = sorted(results, key=lambda r: (r[\"pr_auc\"], r[\"f1\"]), reverse=True)[0]\n",
    "best_name = best[\"model\"]\n",
    "best_pipe = best[\"pipeline\"]\n",
    "\n",
    "print(\"Best model:\", best_name)\n",
    "print(score_table.sort_values(\"pr_auc\", ascending=False).head(10))\n",
    "\n",
    "# Evaluate on TEST\n",
    "test_pred = best_pipe.predict(X_test)\n",
    "\n",
    "if hasattr(best_pipe.named_steps[\"model\"], \"predict_proba\"):\n",
    "    test_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(best_pipe.named_steps[\"model\"], \"decision_function\"):\n",
    "    scores = best_pipe.decision_function(X_test)\n",
    "    test_proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
    "else:\n",
    "    test_proba = None\n",
    "\n",
    "print(\"\\nTEST METRICS\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, test_pred, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, test_pred, zero_division=0))\n",
    "print(\"F1:\", f1_score(y_test, test_pred, zero_division=0))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, test_proba) if test_proba is not None else \"N/A\")\n",
    "print(\"PR-AUC:\", average_precision_score(y_test, test_proba) if test_proba is not None else \"N/A\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, test_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, test_pred, zero_division=0))"
   ],
   "id": "86c6909a1d6ee6f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Optional: simple feature importance if RandomForest was best\n",
    "if best_name == \"RandomForest\":\n",
    "    rf = best_pipe.named_steps[\"model\"]\n",
    "    # After preprocessing, feature names expand due to one-hot.\n",
    "    ohe = best_pipe.named_steps[\"preprocess\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_names = ohe.get_feature_names_out(feature_cols_cat)\n",
    "    all_names = feature_cols_num + list(cat_names)\n",
    "\n",
    "    importances = pd.Series(rf.feature_importances_, index=all_names).sort_values(ascending=False)\n",
    "    display(importances.head(20))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    importances.head(15).sort_values().plot(kind=\"barh\", ax=ax)\n",
    "    ax.set_title(\"Top feature importances (RandomForest)\")\n",
    "    plt.show()"
   ],
   "id": "77689c4047966a10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusions & NASA DEVELOP Framing\n",
    "\n",
    "### What we built\n",
    "A reproducible ML pipeline that predicts **Rapid Intensification (RI)** within the next 24 hours from storm track observations.\n",
    "\n",
    "### What the results mean\n",
    "- Strong validation/test performance suggests the model captures important short-term intensification signals such as:\n",
    "  - current wind intensity\n",
    "  - recent wind trends (6h/12h/24h)\n",
    "  - storm location and system status\n",
    "\n",
    "### Why this matters (hazards / applications)\n",
    "Rapid intensification is operationally challenging and can reduce warning lead time.\n",
    "A lightweight ML classifier can support:\n",
    "- early alerts for potential RI episodes\n",
    "- prioritizing storms for deeper dynamical analysis\n",
    "- decision support for hazard response workflows\n",
    "\n",
    "### Limitations\n",
    "- Label is derived only from wind and track timing; no ocean heat content, shear, SST, satellite imagery, etc.\n",
    "- Dataset is limited to Atlantic best-track; generalization to other basins requires global datasets (IBTrACS).\n",
    "- 24h-ahead “shift -4” assumes regular 6-hour cadence (reasonable for HURDAT2 but still an approximation).\n",
    "\n",
    "### Next Steps (to make this NASA-grade)\n",
    "1. Extend to **IBTrACS** global best-track.\n",
    "2. Add environmental predictors (SST, vertical wind shear, reanalysis).\n",
    "3. Add spatial features (distance to land, coast proximity).\n",
    "4. Use time-series models or gradient boosting with careful leakage control."
   ],
   "id": "18cd86c489b93e95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1fd7a8dec1bc710"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
